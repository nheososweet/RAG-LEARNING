

from langchain_community.document_loaders import TextLoader

# 1.1: Document Ingestion
loader = TextLoader(file_path="inosuke.txt", encoding="utf8")
documents = loader.load()

# 1.2: Text Splitting
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=20,
)

chunks = splitter.split_documents(documents)
for chunk in chunks:
    print(f"Chunk: {chunk.page_content}\n")

# 1.3: Embedding Generation and Vector Store Creation
from langchain_chroma import Chroma
from langchain_huggingface.embeddings import HuggingFaceEmbeddings

embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

vector_store = Chroma(
    collection_name="kimetsu_no_yaiba",
    embedding_function=embedding_function,
    persist_directory="./chroma_db",
)

# vector_store.add_documents(chunks)

query = "Inosuke dÃ¹ng hÆ¡i thá»Ÿ gÃ¬?"
print(f"â“ CÃ¢u há»i: {query}\n")

# 3. TÃŒM KIáº¾M (Similarity Search)
# k=3 nghÄ©a lÃ  láº¥y 3 Ä‘oáº¡n liÃªn quan nháº¥t
results = vector_store.similarity_search(query, k=3)

# 4. HIá»‚N THá»Š Káº¾T QUáº¢
print("--- ğŸ” Káº¾T QUáº¢ TÃŒM KIáº¾M ---")
for i, doc in enumerate(results):
    print(f"\n[Káº¿t quáº£ #{i+1}]")
    print(f"Ná»™i dung: {doc.page_content}")
    print(f"Nguá»“n: {doc.metadata}") # Xem nÃ³ Ä‘áº¿n tá»« file nÃ o